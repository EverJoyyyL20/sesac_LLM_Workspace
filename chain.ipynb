{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ad22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 운영체제(OS) 관련 기능을 사용하기 위한 기본 라이브러리\n",
    "# (환경변수 읽기 위해 필요)\n",
    "# ==============================\n",
    "import os\n",
    "\n",
    "# ==============================\n",
    "# .env 파일을 읽어오기 위한 라이브러리\n",
    "# .env 파일에는 API KEY 같은 민감한 정보를 저장함\n",
    "# ==============================\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# .env 파일에 있는 내용을 실제 환경변수로 등록\n",
    "# ==============================\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 운영체제 환경변수에서 OPENAI_API_KEY 값을 읽어옴\n",
    "# .env 파일 안에 있는 값을 가져오는 역할\n",
    "# ==============================\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d512d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9592\\3816696069.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the `langchain-openai package and should be used instead. To use it run `pip install -U `langchain-openai` and import as `from `langchain_openai import ChatOpenAI``.\n",
      "  llm=ChatOpenAI(temperature=0,\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9592\\3816696069.py:13: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use `RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain=LLMChain(llm=llm,prompt=prompt)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9592\\3816696069.py:14: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain-classic 0.1.0 and will be removed in 1.0. Use `invoke` instead.\n",
      "  chain.run(\"대한민국\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도는 서울입니다.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================\n",
    "# LangChain에서 제공하는 LLMChain 클래스 불러오기\n",
    "# → 프롬프트 + GPT 모델을 연결해서 실행하는 체인\n",
    "# ==============================\n",
    "from langchain_classic.chains import LLMChain\n",
    "\n",
    "# ==============================\n",
    "# PromptTemplate 클래스 불러오기\n",
    "# → 질문 템플릿(틀)을 만들기 위한 도구\n",
    "# ==============================\n",
    "from langchain_classic import PromptTemplate\n",
    "\n",
    "# ==============================\n",
    "# ChatGPT(OpenAI Chat 모델)를 사용하기 위한 클래스 불러오기\n",
    "# ==============================\n",
    "from langchain_classic.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# GPT 모델 객체 생성\n",
    "#\n",
    "# temperature=0\n",
    "# → 답변을 랜덤하지 않게, 가장 정확하게 생성\n",
    "#\n",
    "# model_name='gpt-4.1-mini'\n",
    "# → 사용할 GPT 모델 지정 (빠르고 가벼운 모델)\n",
    "# ==============================\n",
    "llm=ChatOpenAI(temperature=0,\n",
    "               model_name='gpt-4.1-mini')\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 프롬프트 템플릿 생성\n",
    "#\n",
    "# input_variable=[\"country\"]\n",
    "# → 프롬프트 안에서 사용할 변수 이름 지정\n",
    "#\n",
    "# template\n",
    "# → 실제 질문 문장 틀\n",
    "# {country} 부분은 나중에 값이 자동으로 들어감\n",
    "# ==============================\n",
    "prompt=PromptTemplate(\n",
    "    input_variable=[\"country\"],\n",
    "    template=\"{country}의 수도는 어디야?\",\n",
    ")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# LLMChain 생성\n",
    "#\n",
    "# llm=llm\n",
    "# → 위에서 만든 GPT 모델 사용\n",
    "#\n",
    "# prompt=prompt\n",
    "# → 위에서 만든 프롬프트 템플릿 사용\n",
    "#\n",
    "# 즉:\n",
    "# \"이 프롬프트를 이 GPT 모델로 실행하겠다\" 는 의미\n",
    "# ==============================\n",
    "chain=LLMChain(llm=llm,prompt=prompt)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 체인 실행\n",
    "#\n",
    "# \"대한민국\" 값을 PromptTemplate의 {country} 변수에 자동으로 넣음\n",
    "#\n",
    "# 내부적으로:\n",
    "# \"{country}의 수도는 어디야?\"\n",
    "# →\n",
    "# \"대한민국의 수도는 어디야?\"\n",
    "#\n",
    "# 로 변환 후 GPT에게 전달\n",
    "# ==============================\n",
    "chain.run(\"대한민국\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e6c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9592\\3898507903.py:28: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain-classic 0.1.0 and will be removed in 1.0. Use `invoke` instead.\n",
      "  all_chain(sentence)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': '\\nOne limitation of LLMs is their lack of contextual information (e.g., access to some specific documents or emails). You can combat this by giving LLMs access to the specific external data.\\nFor this, you first need to load the external data with a document loader. LangChain provides a variety of loaders for different types of documents ranging from PDFs and emails to websites and YouTube videos.\\n',\n",
       " 'translation': 'LLM의 한 가지 한계는 특정 문서나 이메일과 같은 맥락 정보를 갖고 있지 않다는 점입니다. 이를 해결하려면 LLM이 특정 외부 데이터에 접근할 수 있도록 해야 합니다.  \\n이를 위해 먼저 문서 로더를 사용해 외부 데이터를 불러와야 합니다. LangChain은 PDF, 이메일부터 웹사이트, 유튜브 동영상에 이르기까지 다양한 유형의 문서를 위한 여러 로더를 제공합니다.',\n",
       " 'summary': 'LLM의 한계를 극복하기 위해 LangChain의 다양한 문서 로더를 활용해 PDF, 이메일, 웹사이트, 유튜브 동영상 등 외부 데이터를 불러와 접근할 수 있도록 해야 합니다.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================\n",
    "# 1️⃣ 번역용 프롬프트 템플릿 생성\n",
    "# ==============================\n",
    "prompt1=PromptTemplate(\n",
    "     # 이 프롬프트에서 사용할 입력 변수 이름\n",
    "    # {sentence} 자리에 실제 문장이 들어감\n",
    "    input_variables=['sentence'],\n",
    "    # GPT에게 전달될 질문 템플릿\n",
    "    # \\n\\n 은 줄바꿈 (프롬프트 가독성 증가)\n",
    "    template=\"다음 문장을 한글로 번역하세요 .\\n\\n{sentence}\"\n",
    "\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ 첫 번째 체인 생성 (번역 체인)\n",
    "# ==============================\n",
    "chain1=LLMChain(llm=llm,# 사용할 GPT 모델\n",
    "                prompt=prompt1,# 위에서 만든 번역 프롬프트 사용\n",
    "\n",
    "                # 이 체인의 출력 결과 이름 지정\n",
    "                # 결과가 \"translation\" 이라는 키로 저장됨\n",
    "                output_key='translation')\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ 요약용 프롬프트 템플릿 생성\n",
    "# ==============================\n",
    "prompt2=PromptTemplate.from_template(\n",
    "     # {translation} 은\n",
    "    # chain1 의 output_key 값과 자동 연결됨\n",
    "    \"다음 문장을 한 문장으로 요약하세요 .\\n\\n{translation}\"\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ 두 번째 체인 생성 (요약 체인)\n",
    "# ==============================\n",
    "chain2=LLMChain(llm=llm,# 동일한 GPT 모델 사용\n",
    "                prompt=prompt2, # 요약 프롬프트 사용\n",
    "                 # 이 체인의 출력 결과 이름\n",
    "                output_key='summary')\n",
    "\n",
    "from langchain_classic.chains import SequentialChain\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ 전체 파이프라인 체인 구성\n",
    "# ==============================\n",
    "all_chain=SequentialChain(\n",
    "     # 실행할 체인 순서 지정\n",
    "    # → 먼저 chain1 실행\n",
    "    # → 그 다음 chain2 실행\n",
    "    chains=[chain1,chain2],\n",
    "      # 사용자로부터 처음 입력 받을 변수 이름\n",
    "    # 즉 all_chain() 실행 시 sentence 값을 받아야 함\n",
    "    input_variables=['sentence'],\n",
    "    # 최종 출력으로 받을 결과들\n",
    "    # chain1 결과: translation\n",
    "    # chain2 결과: summary\n",
    "    output_variables=['translation','summary'],\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ 입력 문장 (영어 원문)\n",
    "# ==============================\n",
    "sentence=\"\"\"\n",
    "One limitation of LLMs is their lack of contextual information (e.g., access to some specific documents or emails). You can combat this by giving LLMs access to the specific external data.\n",
    "For this, you first need to load the external data with a document loader. LangChain provides a variety of loaders for different types of documents ranging from PDFs and emails to websites and YouTube videos.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ 전체 체인 실행\n",
    "# ==============================\n",
    "\n",
    "# all_chain(sentence) 실행 시:\n",
    "#\n",
    "# 1) sentence 값이 chain1으로 전달됨\n",
    "# 2) chain1이 번역 수행 → translation 생성\n",
    "# 3) translation 결과가 chain2로 자동 전달\n",
    "# 4) chain2가 요약 수행 → summary 생성\n",
    "# 5) translation + summary 결과 반환\n",
    "all_chain(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a070e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
